dataset: mnist
neuron_params:
  g_leak: 1.0
  g_leak_adapted: 0.0
  leak: 0.0
  tau_syn: 1.0
  tau_mem: 2.0
  threshold: 1.0
network_layout:
  bias_times: []
  layer_sizes: [350, 10]
  n_biases: [0, 0]
  n_inputs: 784
  n_layers: 2
  weight_means: [0.05, 0.15]
  weight_stdevs: [0.8, 0.8]
  # New delay layer parameters
  use_delay_layer: True # Flag to use delay layer
  delay_mean: 0.1 # Mean of the Gaussian distribution for delays
  delay_std: 0.05 # Standard deviation of the Gaussian distribution for delays

training_params:
  alpha: 0.005
  batch_size: 80
  batch_size_eval: 150
  beta: 1.
  enforce_cpu: false
  epoch_number: 150
  epoch_snapshots: [5, 10, 30, 50, 100, 140]
  learning_rate: 0.005
  lr_scheduler: {gamma: 0.90, step_size: 15, type: StepLR}
  max_dw_norm: 0.2
  max_num_missing_spikes: [0.15, 0.05]
  momentum: 0
  numpy_seed: 12345
  optimizer: adam
  print_step_percent: 5.0
  resolution: 0.01
  sim_time: 3.5
  torch_seed: 2345678
  training_noise: {mean: 0.0, std_dev: 0.30}
  use_forward_integrator: true
  use_hicannx: false
  weight_bumping_exp: true
  weight_bumping_targeted: true
  weight_bumping_value: 0.0005
  xi: 0.2
